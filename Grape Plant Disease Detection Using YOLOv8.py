# -*- coding: utf-8 -*-
"""final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fupY5NDXX3whX2WKhFfWdxpZz3UMvvVI
"""

!pip install unzip

!unzip /content/dataset1.zip -d /content/

# this is sample code for reading an image and respective coordinates of the bounding box and printing it
import cv2

# Read the image and get its dimensions
img = cv2.imread("/content/runs/detect/predict/Grape___healthy262.JPG")
h, w = img.shape[:2]

# Read the text content from file and split it into lines
with open("/content/runs/detect/predict/labels/Grape___Leaf_blight987.txt", "r") as f:
    lines = f.read().splitlines()

# Initialize an empty list to store the coordinates
coordinates = []

# Iterate over each line and extract the values
for line in lines:
    values = line.split()
    x, y, nw, nh = map(float, values[1:])

    # Multiply the normalized values by the image dimensions
    xc1 = x * w
    yc1 = y * h
    nw1 = nw * w
    nh1 = nh * h

    # Calculate the top-left and bottom-right coordinates
    top_left = int(xc1 - nw1/2), int(yc1 - nh1/2)
    bottom_right = int(xc1 + nw1/2), int(yc1 + nh1/2)

    # Append the coordinates as a list of 4 values
    coordinates.append([top_left[0], top_left[1], bottom_right[0], bottom_right[1]])

# Print the list of coordinates
print(coordinates)

using_colab = True

# this code is for downloading all the requirements for the segment anything model
if using_colab:
    import torch
    import torchvision
    print("PyTorch version:", torch.__version__)
    print("Torchvision version:", torchvision.__version__)
    print("CUDA is available:", torch.cuda.is_available())
    import sys
    !{sys.executable} -m pip install opencv-python matplotlib
    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'

    !mkdir images
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg

    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import numpy as np
import torch
import matplotlib.pyplot as plt
import cv2

# this function is used to display the segmented region with some random color
def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

# reading an sample image and printing it
img = cv2.imread("/content/dataset_imageslabels/images/train/Grape___Black_rot1.JPG")
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(10,10))
plt.imshow(img)
plt.axis('on')
plt.show()

# setting up the segment anything model
import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor

sam_checkpoint = "sam_vit_h_4b8939.pth"
model_type = "vit_h"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

# this is sample code for converting the coordinates list to the segment anything model format
input_boxes = torch.tensor([
    coordinates
], device=predictor.device)

input_boxes

# we transform the input_boxes so we can give them to the sam predictor
transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, img.shape[:2])

# this is were the input coordinates are taken and diseased area is segmented
masks, _, _ = predictor.predict_torch(
    point_coords=None,
    point_labels=None,
    boxes=transformed_boxes,
    multimask_output=False,
)

# this code is for checking labels.txt file is present for every image or not
import os

image_folder = "/content/runs/detect/predict"
text_folder = "/content/runs/detect/predict/labels"

# Get a list of all image files in the image folder
image_files = [f for f in os.listdir(image_folder) if f.endswith('.JPG')]

# Iterate over each image file
for image_file in image_files:
    # Check if the corresponding text file exists
    text_file = os.path.join(text_folder, os.path.splitext(image_file)[0] + '.txt')
    if not os.path.exists(text_file):
        # If the text file does not exist, create an empty one
        open(text_file, 'a').close()
        print(f"Created empty text file for {image_file}")
    else:
        print(f"Found text file for {image_file}")

import os
import glob
import cv2
import matplotlib.pyplot as plt
import os
import numpy as np
import matplotlib.pyplot as plt
# Path to the images folder and text folder
img_folder = "/content/dataset_imageslabels/images/val"
txt_folder = "/content/dataset_imageslabels/labels/val"
save_dir  = "/content/segmented_val"

# List all image files in the images folder
img_files = glob.glob(os.path.join(img_folder, "*.JPG"))

# Loop through each image file
for img_file in img_files:
    # Read the image and get its dimensions
    img7 = cv2.imread(img_file)
    img7 = cv2.cvtColor(img7, cv2.COLOR_BGR2RGB)
    h, w = img7.shape[:2]

    # Get the corresponding text file
    txt_file = os.path.join(txt_folder, os.path.splitext(os.path.basename(img_file))[0] + ".txt")
    if os.stat(txt_file).st_size == 0:
        # Save a copy of the image as "current image name _segmented.png" inside the "segmented" folder
      filename, ext = os.path.splitext(img_file)
      new_filename = os.path.basename(filename) + '_segmented.png'
      new_filepath = os.path.join(save_dir, new_filename)
      plt.imsave(new_filepath, img7)
      continue
    # Read the text content from file and split it into lines
    with open(txt_file, "r") as f:
        lines = f.read().splitlines()

    # Initialize an empty list to store the coordinates
    coordinates = []

    # Iterate over each line and extract the values
    for line in lines:
        values = line.split()
        x, y, nw, nh = map(float, values[1:])

        # Multiply the normalized values by the image dimensions
        xc1 = x * w
        yc1 = y * h
        nw1 = nw * w
        nh1 = nh * h

        # Calculate the top-left and bottom-right coordinates
        top_left = int(xc1 - nw1/2), int(yc1 - nh1/2)
        bottom_right = int(xc1 + nw1/2), int(yc1 + nh1/2)

        # Append the coordinates as a list of 4 values
        coordinates.append([top_left[0], top_left[1], bottom_right[0], bottom_right[1]])
    #  this line is for setting the image for predection
    predictor.set_image(img7)

    # converting the coordinates list to the input_boxes
    input_boxes = torch.tensor([
    coordinates
    ], device=predictor.device)

# input_boxes are then transformed into the predictor format of the sam model
    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, img7.shape[:2])
    masks, _, _ = predictor.predict_torch(
        point_coords=None,
        point_labels=None,
        boxes=transformed_boxes,
        multimask_output=False,
    )

    masks_np = [mask[0].cpu().numpy() for mask in masks]

    # Create a new numpy array to hold the segmented image, initialized with zeros
    segmented_image = np.zeros_like(masks_np[0])

    # Set the pixels where any of the masks are True to 1 in the `segmented_image` array
    for mask in masks_np:
        segmented_image[np.array(mask, dtype=bool)] = 1

    # Remove the first dimension of `segmented_image` using the `squeeze` method
    segmented_image = segmented_image.squeeze()

    # Apply the mask to the original image using NumPy indexing
    masked_img = img7.copy()
    masked_img[~np.array(segmented_image, dtype=bool)] = 0

    # Get the image filename without the extension
    filename, ext = os.path.splitext(img_file)

    # Construct the new filename for the segmented image
    new_filename = os.path.basename(filename) + '_segmented.png'
    new_filepath = os.path.join(save_dir, new_filename)
    # Save the segmented image to disk using plt.imsave
    plt.imsave(new_filepath, masked_img, cmap='gray', vmin=0, vmax=1)

import os
import glob
import cv2
import matplotlib.pyplot as plt
import os

#  for training the cnn model we need the labelled data
# so all the segmented images are separated into respective folders according to their disease names
import os
import re
import shutil

# Define the source and destination directories
# this location contains the mix of all the segmented images
source_dir = '/content/segmented'
# this location contains the separated segmented images into folders according to their names
dest_dir = '/content/sample_data/seg'

# Define the prefixes
prefixes = ['Grape___Black_rot', 'Grape___Esca', 'Grape___Leaf_blight']

# Create the destination directories
for prefix in prefixes:
    os.makedirs(os.path.join(dest_dir, prefix), exist_ok=True)

print(f"Number of files in {source_dir}: {len(os.listdir(source_dir))}")

# Loop over the files in the source directory
for file_name in os.listdir(source_dir):
    # Extract the prefix from the file name
    prefix_match = re.search(r'^(Grape___(?:Black_rot|Esca|Leaf_blight))\d*_segmented\.png$', file_name)
    if prefix_match:
        prefix = prefix_match.group(1)
        print(prefix)
        if prefix in prefixes:
            # Construct the source and destination paths
            src_path = os.path.join(source_dir, file_name)
            dest_path = os.path.join(dest_dir, prefix, file_name)

            print(f"Copying {src_path} to {dest_path}")

            # Copy the file to the destination directory
            shutil.copy(src_path, dest_path)

# this is for the testing phase

import os
import re
import shutil

# Define the source and destination directories
source_dir = '/content/segmented'
dest_dir = '/content/segmented_test'

# Define the prefixes
prefixes = ['Grape___Black_rot', 'Grape___Esca', 'Grape___Leaf_blight', 'Grape___healthy']

# Create the destination directories
for prefix in prefixes:
    os.makedirs(os.path.join(dest_dir, prefix), exist_ok=True)

print(f"Number of files in {source_dir}: {len(os.listdir(source_dir))}")

# Loop over the files in the source directory
for file_name in os.listdir(source_dir):
    # Extract the prefix from the file name
    prefix_match = re.search(r'^(Grape___(?:Black_rot|Esca|Leaf_blight|healthy))\d*_segmented\.png$', file_name)
    if prefix_match:
        prefix = prefix_match.group(1)
        print(prefix)
        if prefix in prefixes:
            # Construct the source and destination paths
            src_path = os.path.join(source_dir, file_name)
            dest_path = os.path.join(dest_dir, prefix, file_name)

            print(f"Copying {src_path} to {dest_path}")

            # Copy the file to the destination directory
            shutil.copy(src_path, dest_path)

!pip install split-folders

import splitfolders

# Define the input directory
input_dir = '/content/sample_data/seg'

# Define the output directory for the train and validation sets
output_dir = '/content/sample_data/split_data'

# Split the input directory into train and validation sets with a 70/30 split
splitfolders.ratio(input_dir, output=output_dir, seed=42, ratio=(0.7, 0.3))

import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator
!pip install scipy
!pip install scikit-learn

from numpy import expand_dims
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array
from matplotlib import pyplot
import scipy

img = load_img("/content/segmented_test/Grape___Black_rot/Grape___Black_rot1003_segmented.png")
data = img_to_array(img)


aug = ImageDataGenerator(
    rotation_range=25, width_shift_range=0.1,
    height_shift_range=0.1, shear_range=0.5,
    zoom_range=0.4,horizontal_flip=True,
    fill_mode="nearest")
samples = expand_dims(data, 0)


it = aug.flow(samples, batch_size=1)
plt.figure(figsize=(10, 10))
for i in range(9):
	pyplot.subplot(330 + 1 + i)
	batch = it.next()
	image = batch[0].astype('uint')
	pyplot.imshow(image)
pyplot.show()

train_datagen = ImageDataGenerator(
horizontal_flip=True,
width_shift_range=0.1,
height_shift_range=0.1,
shear_range=0.2,
zoom_range=0.1
)

val_datagen = ImageDataGenerator(
    zoom_range=0.01

)

test_datagen = ImageDataGenerator()
test_generator = test_datagen.flow_from_directory(
    directory="/content/segmented_test",
    target_size=(128, 128),
    color_mode="rgb",
    batch_size=32,
    class_mode="categorical",
    shuffle=False
)

train_generator = train_datagen.flow_from_directory(
    directory="/content/segmented_data_split/train",
    target_size=(128, 128),
    color_mode="rgb",
    batch_size=6,
    class_mode="categorical",
    shuffle=True,
    seed=42
)

valid_generator = val_datagen.flow_from_directory(
    directory=r"/content/segmented_data_split/val",
    target_size=(128, 128),
    color_mode="rgb",
    batch_size=6,
    class_mode="categorical",
    seed=42
)

op=train_generator.n//train_generator.batch_size
op2=valid_generator.n//valid_generator.batch_size
print(op,op2)

from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D, AveragePooling2D
from keras.layers import BatchNormalization
from keras.layers.core import Activation, Flatten, Dropout, Dense
model = Sequential()

model.add(Conv2D(32, (3, 3), padding="same",input_shape=(128,128,3)))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3,3), padding = "same"))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3,3), padding = "same"))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3,3), padding = "same"))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))



model.add(Flatten())
model.add(Dense(256))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(4))
model.add(Activation("softmax"))

model.summary()

from tensorflow.keras.optimizers import Adam
opt = Adam(learning_rate =0.0001)
model.compile(loss="categorical_crossentropy", optimizer=opt,metrics=["acc"])

from keras.callbacks import ModelCheckpoint

# Define the checkpoint filepath
checkpoint_filepath = "/content/drive/MyDrive/project/finalproject/checkpoints"

# Define the callback
checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    save_best_only=True,
    monitor='val_loss',
    verbose=1
)

his=model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples//32,
    epochs=500 ,
    validation_data=valid_generator,
    validation_steps=valid_generator.samples//32,
    callbacks=[checkpoint_callback])

import matplotlib.pyplot as plt
acc = his.history['acc']
val_acc = his.history['val_acc']
loss = his.history['loss']
val_loss = his.history['val_loss']
epochs = range(1, len(acc) + 1)
#Train and validation accuracy
plt.plot(epochs, acc, 'b', label='Training accurarcy')
plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
plt.title('Training and Validation accurarcy')
plt.legend()

plt.figure()
#Train and validation loss
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation loss')
plt.legend()
plt.show()

model.save("/content/drive/MyDrive/project/finalproject/models/model_500_63.h5")

scores = model.evaluate_generator(generator=valid_generator)
print(f"Test Accuracy: {scores[1]*100}")

import numpy as np
from tensorflow.keras.preprocessing import image

# Load the image you want to make a prediction on
img_path = "/content/sample_data/seg/Grape___Black_rot/Grape___Black_rot11_segmented.png"
img = image.load_img(img_path, target_size=(128, 128))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)

# Make the prediction using the trained model
predictions = model.predict(x)

# Print the predicted class probabilities
print(predictions)

import numpy as np
from tensorflow.keras.preprocessing import image

# Load the image you want to make a prediction on
img_path = "/content/sample_data/seg/Grape___healthy/Grape___healthy2.JPG"
img = image.load_img(img_path, target_size=(128, 128))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)

# Make the prediction using the trained model
predictions = model.predict(x)

# Print the predicted class probabilities
print(predictions)

# Get the class indices from the train_generator object
class_indices = train_generator.class_indices
print(class_indices)
# Get the predicted class index
predicted_class_index = np.argmax(predictions, axis=1)[0]

# Get the predicted class name
predicted_class_name = list(class_indices.keys())[list(class_indices.values()).index(predicted_class_index)]

# Print the predicted class name
print(predicted_class_name)



# this is for testing phase

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics
import ultralytics
ultralytics.checks()

!yolo train model=yolov8x.pt data=/content/dataset.yaml epochs=100 imgsz=640

!unzip /content/dataset1.zip -d /content/

!pip install ultralytics -q
!pip install pyyaml -q

from ultralytics import YOLO
import yaml
import cv2
from google.colab.patches import cv2_imshow

model = YOLO("/content/drive/MyDrive/finalproject/detect/train/weights/best.pt")

model.predict("/content/testdata" , save = True , save_txt = True)

model = keras.models.load_model('/content/drive/MyDrive/project/finalproject/models/model_500_63.h5')

from sklearn.metrics import classification_report

true_labels = test_generator.classes
predicted_probs = model.predict(test_generator)

predicted_labels = np.argmax(predicted_probs, axis=1)

print(classification_report(true_labels, predicted_labels))

from sklearn.metrics import confusion_matrix

true_labels = test_generator.classes
predicted_probs = model.predict(test_generator)
predicted_labels = np.argmax(predicted_probs, axis=1)

cm = confusion_matrix(true_labels, predicted_labels)
print(cm)

import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, f1_score

# Load the test data using an ImageDataGenerator

# Get the true labels for the test data
true_labels = test_generator.classes

# Predict the labels for the test data using your deep learning model
predicted_probs = model.predict(test_generator)
predicted_labels = np.argmax(predicted_probs, axis=1)

# Calculate the F1 score and print the classification report
f1 = f1_score(true_labels, predicted_labels, average='weighted')

print("F1 Score: ", f1)
print(classification_report(true_labels, predicted_labels))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import itertools

true_labels = test_generator.classes
predicted_probs = model.predict(test_generator)
predicted_labels = np.argmax(predicted_probs, axis=1)

cm = confusion_matrix(true_labels, predicted_labels)

# Define the class labels
class_names = ['class 0', 'class 1', 'class 2', 'class 3']

# Create a function to plot the confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Plot the confusion matrix
plot_confusion_matrix(cm, classes=class_names, normalize=False,
                      title='Confusion matrix')
plt.show()